export CMT_HOME=~/cmt
export CMT_USER=$(whoami)
export CMT_SSH_PUBLIC_KEY=~/.ssh/cmt_id_rsa.pub

export COMPOSE_DOCKER_CLI_BUILD=1
export DOCKER_BUILDKIT=1

function rc-db-host () {
    ssh rc-utility.us-east-1.cmtaws 'source ~/vtrack.conf && echo $RDSHOST' | xargs | sed 's/ //g'
}

function rc-db-pw () {
    ssh rc-utility.us-east-1.cmtaws 'source ~/vtrack.conf && aws rds generate-db-auth-token --hostname "$RDSHOST" --port 5432 --username vtrack'
}

function rc-db-tunnel () {
    export PGUSER=vtrack
    export PGDATABASE=vtrack
    export RDSHOST=$(rc-db-host < /dev/null)
    export PGHOST=localhost
    export PGPASSWORD=$(rc-db-pw < /dev/null)
    cmtaws tunnel --tunnel-timeout 30 --bind-port-env-var PGPORT localhost:0:"$RDSHOST":5432 rc-utility.us-east-1.cmtaws -- "$@"
}

alias ce='cmtaws ec2'
alias cs='cmtaws sqs'

function launch_worker() {
    assume vision
    cd ~/cmt/vtrack
    git fetch
    version=$(latest_tag)
    if [[ $# -ge 1 ]]
    then
        version=$1
    fi
    count=$(ce list -n rc- | grep "rc-par-" | wc -l | xargs)
    #count=$(($count - 3))
    #vmsize="m4.2xlarge"
    vmsize="m4.large"
    #vmsize="t3.medium"
    aws --region us-east-1 ec2 run-instances --launch-template LaunchTemplateName=rc-worker-reserved --tag-specifications "ResourceType=instance,Tags=[{Key=asg_build_version,Value=${version}},{Key=queue,Value=rc_par},{Key=Name,Value=rc-par-worker-${count}}]" --subnet-id subnet-e59c3292 --instance-type m4.2xlarge --count 1
}

function latest_tag () {
    cd ~/cmt/vtrack
    git tag --sort=-creatordate  | grep "^v20" |  grep production | head -n 1
}

function run_rc_test() {( set -e
    cd ~/cmt/vtrack
    CURRENT_BRANCH=$(git rev-parse --abbrev-ref HEAD)

    echo "running rc test on $CURRENT_BRANCH"

    # role
    assume vision

    # fetch vtrack to get the latest tag
    echo "Fetch vtrack repo? (y/n)"
    read dofetch
    if [ "$dofetch" = "y" ]; then
        git fetch
    fi


    # lock Pipfile
    $(cmtaws codeartifact read)

    echo "Compute lock? (y = lock / n = skip locking / p = lock with --pre)"
    read doprelock
    if [ "$doprelock" = "p" ]; then
        mm/x lock_pre || mm/x lock_pre  # looks like I have to lock twice sometimes

        echo "Commit (y/n)"
        read docommit
        if [ "$docommit" = "p" ]; then
            git add mm/Pipfile
            git add mm/Pipfile.lock
            git commit -m "pipfile lock --pre"
        fi
    elif [ "$doprelock" = "y" ]; then
        mm/x lock
        echo "Commit (y/n)"
        read docommit
        if [ "$docommit" = "p" ]; then
            git add mm/Pipfile
            git add mm/Pipfile.lock
            git commit -m "pipfile lock"
        fi
    fi


    # tag the branch
    BASE_TAG=$(latest_tag)
    VTAG=$BASE_TAG
    ctr=0

    while [ $(git tag -l "$VTAG") ]; do
        echo "$VTAG already exists, incrementing"
        VTAG=$(echo $BASE_TAG | sed "s/production.*/par.$ctr/g")
        ((ctr=ctr+1))
    done

    echo "going to run rc test with tag $VTAG"

    echo "Tag branch $CURRENT_BRANCH with tag $VTAG? (y/n)"
    read branchtag
    if [ "$branchtag" = "y" ]; then
        git tag $VTAG
        git push origin $VTAG
    fi

    echo "Launch worker to test on? (y/n)"
    read launchworker
    if [ "$launchworker" = "y" ]; then
        launch_worker $VTAG
    fi


    echo "Run rc test on branch $CURRENT_BRANCH with tag $VTAG? (y/n)"
    read runtest
    if [ "$runtest" = "y" ]; then
        # run the test
        rc-db-tunnel rc-helper launch --branch $CURRENT_BRANCH --tag=$VTAG --skip-sanity-check --instance-type c5.4xlarge --instance-count 30 --sqs-profile vision --ec2-profile vision
    fi


    echo "results will be at the following URL"
    echo "https://my-rc.cmtelematics.com/vcompare.php?versionA=$BASE_TAG&versionB=$VTAG&filter=tag_rctest_20230804%20notnull&chartBackend=psql&clearQueryCache=yes&comparisonType=identical"

)}


##################################################
## some cmt aliases

alias csp='cmtaws sqs list -n par'
alias cep='cmtaws ec2 list -n par'
function launch_rc_par {
    if [ $AWS_PROFILE = "sfdev" ]; then
        aws --region us-east-1 ec2 run-instances --launch-template LaunchTemplateName=dssf-prod-worker --tag-specifications "ResourceType=instance,Tags=[{Key=Name,Value=par-worker}]" --subnet-id subnet-06a3978e034d06081 --instance-type m5.4xlarge --count 1
    else
        aws --region us-east-1 ec2 run-instances --launch-template LaunchTemplateName=rc-worker-reserved --tag-specifications "ResourceType=instance,Tags=[{Key=asg_build_version,Value=$1},{Key=queue,Value=rc_par},{Key=Name,Value=rc-par-worker}]" --subnet-id subnet-e59c3292 --instance-type t2.medium --count 1
    fi

}

function clean_rc_queues {
    cmtaws sqs list -n rc_v4_ --nocolor | tail -n +3 | awk '{print $1}' | grep rc_v4 --color='never' | sed 's/in-rc_//g' | sed 's/_/./g' | xargs -I{} -L1 rc-helper end --tag {}
}

function cmt_start {
    export CMT_USER=$USER
    cmtaws sso login
    assume dev-user
    cmtaws ecr login
    aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin 173754725891.dkr.ecr.us-east-1.amazonaws.com
    $(cmtaws codeartifact read)
}

function cmt_run_vtrack_pipeline {
    assume vision

    # pull image
    cmtaws ecr login
    docker pull 062438643287.dkr.ecr.us-east-1.amazonaws.com/cmt-dev/vtrack-mm:master-cache
    $(cmtaws codeartifact read)
    docker image ls

    # setup env
    docker-compose -f $CMT_HOME/vtrack/docker/docker-compose.yml up -d
    # enter container
    echo "You probably want to run ./docker/run_test_step_accelpipeline.sh"
    docker-compose -f $CMT_HOME/vtrack/docker/docker-compose.yml exec unittest bash -c "./docker/run_test_step_accelpipeline.sh"
    docker-compose -f $CMT_HOME/vtrack/docker/docker-compose.yml exec unittest bash

    # after container exit, shutdown
    echo "Going to shut down container in 10sec"
    sleep 10
    docker-compose -f $CMT_HOME/vtrack/docker/docker-compose.yml down
}

function nocolor {
    # for bsd systems
    while read -r foo; do
        echo $foo | sed $'s,\x1b\\[[0-9;]*[a-zA-Z],,g'
    done

    # for gnu-sed:  sed -r "s/\x1B\[([0-9]{1,3}(;[0-9]{1,2})?)?[mGK]//g"
}

function assume {
    local -a subcmds
    subcmds=($(cmtaws sso list > /dev/null 2>&1 | nocolor | tail -n+3 | awk '{print $1}'))
    if [[ " ${subcmds[@]} " =~ " ${1} " ]]; then
        export AWS_PROFILE=$1
    else
        echo "ERROR: Role '$1' not found. Possible roles are (${subcmds})" 1>&2
        return 1
    fi
}


function setupEMRNode {
    sudo yum install -y zsh emacs-nox tmux htop
    echo "export SPARK_HOME=/usr/lib/spark" >> ~/.zshrc
    echo 'export PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/build:$SPARK_HOME/python/lib/py4j-0.10.9-src.zip:$PYTHONPATH' >> ~/.zshrc
    echo "export PYSPARK_PYTHON=$(which python3)" >> ~/.zshrc
    echo "export PYSPARK_DRIVER_PYTHON=$(which python3)" >> ~/.zshrc
    pip3 install -U s3fs boto3 tabulate ipdb --user
}

function _assume_comp {
    local -a subcmds
    subcmds=($(cmtaws sso list > /dev/null 2>&1 | nocolor | tail -n+3 | awk '{print $1}'))
    _describe 'assume' subcmds
}
compdef _assume_comp assume


function launch_emr_cluster {
    if [ $AWS_PROFILE = "vision" ]; then
        aws emr create-cluster --applications Name=Hadoop Name=Spark Name=Livy Name=Ganglia Name=JupyterHub --tags 'token=analytics' --ec2-attributes '{"AdditionalSlaveSecurityGroups":["sg-03b8a4c1b01e3b8ef","sg-064bd34250465a54e"],"InstanceProfile":"EMR_EC2_DefaultRole","ServiceAccessSecurityGroup":"sg-0ebb2ef01cf5113c0","SubnetId":"subnet-04bf1e12bfe462d3b","EmrManagedSlaveSecurityGroup":"sg-0b772c4d67e1e1a2f","EmrManagedMasterSecurityGroup":"sg-0800bbdfcf7fe8d07","AdditionalMasterSecurityGroups":["sg-03b8a4c1b01e3b8ef","sg-064bd34250465a54e"]}' --release-label emr-5.31.0 --log-uri 's3n://cmt-analytics-dssf-prod/aws/emr/app-logs/' --instance-groups '[{"InstanceCount":1,"BidPrice":"OnDemandPrice","EbsConfiguration":{"EbsBlockDeviceConfigs":[{"VolumeSpecification":{"SizeInGB":128,"VolumeType":"gp2"},"VolumesPerInstance":4}],"EbsOptimized":true},"InstanceGroupType":"MASTER","InstanceType":"r4.2xlarge","Configurations":[{"Classification":"core-site","Properties":{"hadoop.proxyuser.livy.groups":"*","hadoop.proxyuser.livy.hosts":"*"}},{"Classification":"hive-site","Properties":{"hive.metastore.schema.verification":"false"}},{"Classification":"jupyter-s3-conf","Properties":{"s3.persistence.bucket":"cmt-analytics-dssf-prod","s3.persistence.enabled":"true"}},{"Classification":"livy-conf","Properties":{"livy.server.session.timeout-check":"true","livy.server.yarn.app-lookup-timeout":"600s","livy.impersonation.enabled":"true"}},{"Classification":"mapred-site","Properties":{"mapreduce.job.maxtaskfailures.per.tracker":"20"}},{"Classification":"spark-defaults","Properties":{"spark.yarn.scheduler.reporterThread.maxFailures":"4","spark.dynamicAllocation.maxExecutors":"200","spark.dynamicAllocation.enabled":"true"}}],"Name":"MASTER"},{"InstanceCount":1,"BidPrice":"OnDemandPrice","EbsConfiguration":{"EbsBlockDeviceConfigs":[{"VolumeSpecification":{"SizeInGB":256,"VolumeType":"gp2"},"VolumesPerInstance":4}],"EbsOptimized":true},"InstanceGroupType":"CORE","InstanceType":"r4.2xlarge","Configurations":[{"Classification":"core-site","Properties":{"hadoop.proxyuser.livy.groups":"*","hadoop.proxyuser.livy.hosts":"*"}},{"Classification":"hive-site","Properties":{"hive.metastore.schema.verification":"false"}},{"Classification":"jupyter-s3-conf","Properties":{"s3.persistence.bucket":"cmt-analytics-dssf-prod","s3.persistence.enabled":"true"}},{"Classification":"livy-conf","Properties":{"livy.server.session.timeout-check":"true","livy.server.yarn.app-lookup-timeout":"600s","livy.impersonation.enabled":"true"}},{"Classification":"mapred-site","Properties":{"mapreduce.job.maxtaskfailures.per.tracker":"20"}},{"Classification":"spark-defaults","Properties":{"spark.yarn.scheduler.reporterThread.maxFailures":"4","spark.dynamicAllocation.maxExecutors":"200","spark.dynamicAllocation.enabled":"true"}}],"Name":"CORE"}]' --configurations '[{"Classification":"yarn-site","Properties":{"yarn.resourcemanager.nodemanager-graceful-decommission-timeout-secs":"300090","yarn.nodemanager.pmem-check-enabled":"false","yarn.nodemanager.vmem-check-enabled":"false"}},{"Classification":"livy-conf","Properties":{"livy.server.session.timeout":"13h"}},{"Classification":"capacity-scheduler","Properties":{"yarn.scheduler.capacity.root.default.accessible-node-labels.cmtCOMPACT.capacity":"100","yarn.scheduler.capacity.default.minimum-user-limit-percent":"100","yarn.scheduler.capacity.root.capacity":"100","yarn.scheduler.capacity.maximum-am-resource-percent":"0.9","yarn.scheduler.capacity.maximum-applications":"1000","yarn.scheduler.capacity.root.accessible-node-labels.cmtCOMPACT.capacity":"100","yarn.scheduler.capacity.root.accessible-node-labels.cmtTASK.capacity":"100","yarn.scheduler.capacity.root.default.accessible-node-labels.cmtTASK.capacity":"100","yarn.scheduler.capacity.resource-calculator":"org.apache.hadoop.yarn.util.resource.DominantResourceCalculator","yarn.scheduler.capacity.root.default.capacity":"100"}},{"Classification":"spark-env","Properties":{"SPARK_DAEMON_MEMORY":"4g"},"Configurations":[{"Classification":"export","Properties":{"PYSPARK_PYTHON":"/usr/bin/python3","PYTHONPATH":"$HOME/cmt:$HOME/cmt/vtrack:$HOME/cmt/vtrack/base:$HOME/cmt/vtrack/lib:$HOME/cmt/vtrack/mm:$PYTHONPATH"}}]},{"Classification":"spark","Properties":{"maximizeResourceAllocation":"true"}},{"Classification":"spark-defaults","Properties":{"spark.yarn.scheduler.reporterThread.maxFailures":"5","spark.driver.maxResultSize":"0","spark.executor.heartbeatInterval":"10000000s","spark.rdd.compress":"true","spark.network.timeout":"10000001s","spark.memory.storageFraction":"0.30","spark.dynamicAllocation.maxExecutors":"1000","spark.shuffle.spill.compress":"true","spark.shuffle.compress":"true","spark.storage.level":"MEMORY_AND_DISK_SER","spark.serializer":"org.apache.spark.serializer.KryoSerializer","spark.dynamicAllocation.minExecutors":"1","spark.memory.fraction":"0.80","spark.executor.extraJavaOptions":"-XX:+UseG1GC -XX:+UnlockDiagnosticVMOptions -XX:+G1SummarizeConcMark -XX:InitiatingHeapOccupancyPercent=35 -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:OnOutOfMemoryError='\''kill -9 %p'\''","spark.dynamicAllocation.enabled":"false","spark.driver.extraJavaOptions":"-XX:+UseG1GC -XX:+UnlockDiagnosticVMOptions -XX:+G1SummarizeConcMark -XX:InitiatingHeapOccupancyPercent=35 -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:OnOutOfMemoryError='\''kill -9 %p'\''"}},{"Classification":"presto-connector-hive","Properties":{"hive.metastore":"glue"}},{"Classification":"spark-hive-site","Properties":{"hive.metastore.client.factory.class":"com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory"}}]' --auto-scaling-role EMR_AutoScaling_DefaultRole --bootstrap-actions '[{"Path":"s3://cmtaws/aws/emr/cmt_emr_py3_bootstrap.sh","Name":"Upload CMT Repos"},{"Path":"s3://cmtaws/aws/emr/cmt_emr_data_jars.sh","Name":"Add CMT Jars"}]' --ebs-root-volume-size 100 --service-role EMR_DefaultRole --enable-debugging --name 'par-emr-small-2' --scale-down-behavior TERMINATE_AT_TASK_COMPLETION --region us-east-1
    elif [ $AWS_PROFILE = "sfdev" ]; then
        aws emr create-cluster --applications Name=Hadoop Name=Spark Name=Livy Name=Ganglia Name=JupyterHub --tags 'token=analytics' --ec2-attributes '{"AdditionalSlaveSecurityGroups":["sg-03b8a4c1b01e3b8ef","sg-064bd34250465a54e"],"InstanceProfile":"EMR_EC2_DefaultRole","ServiceAccessSecurityGroup":"sg-0ebb2ef01cf5113c0","SubnetId":"subnet-04bf1e12bfe462d3b","EmrManagedSlaveSecurityGroup":"sg-0b772c4d67e1e1a2f","EmrManagedMasterSecurityGroup":"sg-0800bbdfcf7fe8d07","AdditionalMasterSecurityGroups":["sg-03b8a4c1b01e3b8ef","sg-064bd34250465a54e"]}' --release-label emr-5.31.0 --log-uri 's3n://cmt-analytics-dssf-prod/aws/emr/app-logs/' --instance-groups '[{"InstanceCount":1,"BidPrice":"OnDemandPrice","EbsConfiguration":{"EbsBlockDeviceConfigs":[{"VolumeSpecification":{"SizeInGB":256,"VolumeType":"gp2"},"VolumesPerInstance":4}],"EbsOptimized":true},"InstanceGroupType":"CORE","InstanceType":"r4.2xlarge","Configurations":[{"Classification":"core-site","Properties":{"hadoop.proxyuser.livy.groups":"*","hadoop.proxyuser.livy.hosts":"*"}},{"Classification":"hive-site","Properties":{"hive.metastore.schema.verification":"false"}},{"Classification":"jupyter-s3-conf","Properties":{"s3.persistence.bucket":"cmt-analytics-dssf-prod","s3.persistence.enabled":"true"}},{"Classification":"livy-conf","Properties":{"livy.server.session.timeout-check":"true","livy.server.yarn.app-lookup-timeout":"600s","livy.impersonation.enabled":"true"}},{"Classification":"mapred-site","Properties":{"mapreduce.job.maxtaskfailures.per.tracker":"20"}},{"Classification":"spark-defaults","Properties":{"spark.yarn.scheduler.reporterThread.maxFailures":"4","spark.dynamicAllocation.maxExecutors":"200","spark.dynamicAllocation.enabled":"true"}}],"Name":"CORE"},{"InstanceCount":1,"BidPrice":"OnDemandPrice","EbsConfiguration":{"EbsBlockDeviceConfigs":[{"VolumeSpecification":{"SizeInGB":128,"VolumeType":"gp2"},"VolumesPerInstance":4}],"EbsOptimized":true},"InstanceGroupType":"MASTER","InstanceType":"r4.2xlarge","Configurations":[{"Classification":"core-site","Properties":{"hadoop.proxyuser.livy.groups":"*","hadoop.proxyuser.livy.hosts":"*"}},{"Classification":"hive-site","Properties":{"hive.metastore.schema.verification":"false"}},{"Classification":"jupyter-s3-conf","Properties":{"s3.persistence.bucket":"cmt-analytics-dssf-prod","s3.persistence.enabled":"true"}},{"Classification":"livy-conf","Properties":{"livy.server.session.timeout-check":"true","livy.server.yarn.app-lookup-timeout":"600s","livy.impersonation.enabled":"true"}},{"Classification":"mapred-site","Properties":{"mapreduce.job.maxtaskfailures.per.tracker":"20"}},{"Classification":"spark-defaults","Properties":{"spark.yarn.scheduler.reporterThread.maxFailures":"4","spark.dynamicAllocation.maxExecutors":"200","spark.dynamicAllocation.enabled":"true"}}],"Name":"MASTER"}]' --configurations '[{"Classification":"yarn-site","Properties":{"yarn.resourcemanager.nodemanager-graceful-decommission-timeout-secs":"300090","yarn.nodemanager.pmem-check-enabled":"false","yarn.nodemanager.vmem-check-enabled":"false"}},{"Classification":"livy-conf","Properties":{"livy.server.session.timeout":"13h"}},{"Classification":"capacity-scheduler","Properties":{"yarn.scheduler.capacity.root.default.accessible-node-labels.cmtCOMPACT.capacity":"100","yarn.scheduler.capacity.default.minimum-user-limit-percent":"100","yarn.scheduler.capacity.root.capacity":"100","yarn.scheduler.capacity.maximum-am-resource-percent":"0.9","yarn.scheduler.capacity.maximum-applications":"1000","yarn.scheduler.capacity.root.accessible-node-labels.cmtCOMPACT.capacity":"100","yarn.scheduler.capacity.root.accessible-node-labels.cmtTASK.capacity":"100","yarn.scheduler.capacity.root.default.accessible-node-labels.cmtTASK.capacity":"100","yarn.scheduler.capacity.resource-calculator":"org.apache.hadoop.yarn.util.resource.DominantResourceCalculator","yarn.scheduler.capacity.root.default.capacity":"100"}},{"Classification":"spark-env","Properties":{"SPARK_DAEMON_MEMORY":"4g"},"Configurations":[{"Classification":"export","Properties":{"PYSPARK_PYTHON":"/usr/bin/python3","PYTHONPATH":"$HOME/cmt:$HOME/cmt/vtrack:$HOME/cmt/vtrack/base:$HOME/cmt/vtrack/lib:$HOME/cmt/vtrack/mm:$PYTHONPATH"}}]},{"Classification":"spark","Properties":{"maximizeResourceAllocation":"true"}},{"Classification":"spark-defaults","Properties":{"spark.yarn.scheduler.reporterThread.maxFailures":"5","spark.driver.maxResultSize":"0","spark.executor.heartbeatInterval":"10000000s","spark.rdd.compress":"true","spark.network.timeout":"10000001s","spark.memory.storageFraction":"0.30","spark.dynamicAllocation.maxExecutors":"1000","spark.shuffle.spill.compress":"true","spark.shuffle.compress":"true","spark.storage.level":"MEMORY_AND_DISK_SER","spark.serializer":"org.apache.spark.serializer.KryoSerializer","spark.dynamicAllocation.minExecutors":"1","spark.memory.fraction":"0.80","spark.executor.extraJavaOptions":"-XX:+UseG1GC -XX:+UnlockDiagnosticVMOptions -XX:+G1SummarizeConcMark -XX:InitiatingHeapOccupancyPercent=35 -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:OnOutOfMemoryError='\''kill -9 %p'\''","spark.dynamicAllocation.enabled":"false","spark.driver.extraJavaOptions":"-XX:+UseG1GC -XX:+UnlockDiagnosticVMOptions -XX:+G1SummarizeConcMark -XX:InitiatingHeapOccupancyPercent=35 -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:OnOutOfMemoryError='\''kill -9 %p'\''"}},{"Classification":"presto-connector-hive","Properties":{"hive.metastore":"glue"}},{"Classification":"spark-hive-site","Properties":{"hive.metastore.client.factory.class":"com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory"}}]' --auto-scaling-role EMR_AutoScaling_DefaultRole --bootstrap-actions '[{"Path":"s3://cmt-analytics-dssf-prod/dev/pareshmg/emr/cmt_emr_py3_bootstrap.sh","Name":"Upload CMT Repos"},{"Path":"s3://cmt-analytics-dssf-prod/dev/pareshmg/emr/cmt_emr_data_jars.sh","Name":"Add CMT Jars"}]' --ebs-root-volume-size 100 --service-role EMR_DefaultRole --enable-debugging --name 'par-emr-small-2' --scale-down-behavior TERMINATE_AT_TASK_COMPLETION --region us-east-1

    fi
}

function awsdns {
    if [[ "${1}" == "cmtgpu" ]]; then
        echo ${1}
    else
        accnt=$(getaccnt)
        reg=$(get_aws_region $1)
        echo ${1}.${reg}.${AWS_PROFILE}.cmtaws
    fi
}
function getaccnt {
    if [[ "$AWS_PROFILE" == *"stg"* ]]; then
        echo "stg"
    elif [[ "$AWS_PROFILE" == *"sf"* ]]; then
        echo "cmt.ssm"
    else
        echo "cmt"
    fi
}
function awsid {
    local reg=$(get_aws_region $1)
    local vname=$1
         local pdns=$(cmtaws ec2 -n $@ -e --detail --nocolor -r $reg | awk -v target=id 'BEGIN{
    OFS=FS=" "
    split(target,fields,FS)
    for (i in fields)
        field_idx[fields[i]] = i
}
NR==1 {
    for (i=1;i<=NF;i++)
        head[i] = $i
    next
}
{
    sep=""
    for (i=1;i<=NF;i++)
        if (head[i] in field_idx) {
            printf "%s%s",sep,$i
            sep=OFS
    }
    printf "\n"
}'  | head -n 2 | tail -n+2)

    #local pdns="$(aws ec2 describe-instances --filters Name=tag:Name,Values=${vname} | grep -i publicdnsname | head -n 1 | tr -d '\"' | awk '{print $2}' | tr -d ',')"
    echo ${pdns}
}

function get_aws_region {
    fname="/tmp/aws_regions.cache"
    touch $fname
    found=false
    resregion='us-east-1'
    while IFS='' read -r line || [[ -n "$line" ]]; do
        a=$(echo "$line" | awk '{print $1}')
        r=$(echo "$line" | awk '{print $2}')
        if [ "$a" = "$1" ]; then
            resregion=$r
            found=true
            break
        fi
    done < "$fname"
    if [ "$found" = true ]; then
        echo ${resregion}
        return
    fi

    for region in $(aws ec2 describe-regions --query "Regions[*].[RegionName]" --output text | tac)
    do
        #echo -e "\nListing Instances in region:'$region'..."
        if  [[ $(aws ec2 describe-instances --region $region --filters "Name=tag:Name,Values=$1*"  --query "Reservations[*].Instances[*].[Tags[?Key=='Name'].Value]" --output text | wc -c) -ne 0 ]]; then
            resregion=$region
            break
        fi

        if  [[ $(aws ec2 describe-instances --region $region --filters "Name=instance-id,Values=*$1"  --query "Reservations[*].Instances[*].[InstanceId]" --output text | wc -c) -ne 0 ]]; then
            resregion=$region
            break
        fi
    done
    echo ${resregion}
    echo "$1 $resregion" >> $fname
}

function sshaws_old {
    accnt=$(getaccnt)
    reg=$(get_aws_region $1)
    yes | cmtaws ec2 putsshkey --pubkey ${CMT_ID_RSA}.pub -n $1 -e -r $reg -s running  || yes | cmtaws ec2 putsshkey --pubkey ${CMT_ID_RSA}.pub -i $1 -e -r $reg -s running
    TMUX_ADDITIONAL_CMD=""
    if [ ! -z "$CMT_TMUX_PREFIX" ]; then
        TMUX_ADDITIONAL_CMD="\\; unbind-key \"C-b\" \\; bind-key ${CMT_TMUX_PREFIX} send-prefix \\; set -g prefix ${CMT_TMUX_PREFIX}"
    fi
    instance_id=$(cmtaws ec2 list -n $1 -r $reg -e --nocolor -s running | head -n3 | tail -n1 | awk '{print $1}')
    if [[ $(echo ${instance_id} | wc -c) -lt 2 ]]; then
        instance_id=$(cmtaws ec2 list -i $1 -r $reg -e --nocolor -s running | head -n3 | tail -n1 | awk '{print $1}')
    fi
    cmd="tmux -L ${USER} attach -dt ${USER} || tmux -L ${USER} new-session -s ${USER} ${TMUX_ADDITIONAL_CMD} || tmux -L ${USER} -t ${USER} ${TMUX_ADDITIONAL_CMD}"
    echo ${instance_id}
    echo "ssh ${@:2} -A -t ${instance_id}.${reg}.${accnt} ${cmd}"
    title "${1}.${reg}" && ssh ${@:2} -A -t ${instance_id}.${reg}.${accnt} ${cmd}
}
function sshaws {
    accnt="$AWS_PROFILE.cmtaws"
    TARGET_HOST=$(echo $1 | awk -F"@" '{print $NF}')
    reg=$(get_aws_region $TARGET_HOST)
    TARGET_INSTANCE_USER=$(echo $1 | grep "@" | awk -F"@" '{print $1}')
    echo $TARGET_INSTANCE_USER
    if [ -z $TARGET_INSTANCE_USER ]; then
        TARGET_INSTANCE_USER="ubuntu"
    fi
    echo "$TARGET_INSTANCE_USER@$TARGET_HOST"

    echo "region: $reg"
    TARGET_INSTANCE_ID=$(cmtaws ec2 list -n ${TARGET_HOST} -r $reg -e --nocolor -s running | head -n3 | tail -n1 | awk '{print $1}')
    if [[ $(echo ${TARGET_INSTANCE_ID} | wc -c) -lt 2 ]]; then
        TARGET_INSTANCE_ID=$(cmtaws ec2 list -i ${TARGET_HOST} -r $reg -e --nocolor -s running | head -n3 | tail -n1 | awk '{print $1}')
    fi
    echo "TARGET_INSTANCE_ID: ${TARGET_INSTANCE_ID}"

    cmtaws ec2 putsshkey -i $HOST -r $reg
    #aws --region "$reg" ssm send-command --document-name CMT-PutSSHKey --query 'Command.{CommandId: CommandId, TargetCount: TargetCount, Status: Status}' --parameters "key=$(cat $CMT_SSH_PUBLIC_KEY),user=$TARGET_INSTANCE_USER" --instance-ids "$TARGET_INSTANCE_ID" > /dev/null

    TMUX_ADDITIONAL_CMD=""
    if [ ! -z "$CMT_TMUX_PREFIX" ]; then
        TMUX_ADDITIONAL_CMD="\\; unbind-key \"C-b\" \\; bind-key ${CMT_TMUX_PREFIX} send-prefix \\; set -g prefix ${CMT_TMUX_PREFIX}"
    fi
    TUSER=$(whoami)
    cmd="tmux -L ${USER} -f ~/.tmux.conf.${TUSER} attach -dt ${USER} || tmux -L ${USER} -f ~/.tmux.conf.${TUSER} new-session -s ${USER} ${TMUX_ADDITIONAL_CMD} || tmux -L ${USER} -f ~/.tmux.conf.${TUSER} -t ${USER} ${TMUX_ADDITIONAL_CMD}"

    if [ -z ${SSHAWS_NO_TMUX} ]; then
        scp ~/.tmux.conf ${TARGET_INSTANCE_USER}@${TARGET_INSTANCE_ID}.${reg}.${accnt}:\~/.tmux.conf.${TUSER}
        title "${1}.${reg}" && ssh_color ${TARGET_INSTANCE_USER}@${TARGET_INSTANCE_ID}.${reg}.${accnt} -t -A ${@:2} ${cmd}
    else
        ssh_color ${TARGET_INSTANCE_USER}@${TARGET_INSTANCE_ID}.${reg}.${accnt} -t -A ${@:2}
    fi
}


function cleanupAwsDns {
    local pdns="$(echo $1 | grep -i publicdnsname | tr -d '\"' | awk '{print $2}' | tr -d ',')"
    echo ${pdns}
}


function getSubnets {
         arr1=$(aws ec2 describe-subnets --filters "Name=tag:Name,Values=CMT Instances *" | grep SubnetId | sed "s/\"//g"  )
         arr2=$(aws ec2 describe-subnets --filters "Name=tag:Name,Values=CMT Instances *" | grep "CMT Instances" | sed "s/\"//g" )
         nl=$(echo $arr1 | wc -l)
         for (( i=0; i<nl; ++i));
         do
            v1=$(echo $arr1 | sed -n "${i}p")
            v2=$(echo $arr2 | sed -n "${i}p")
            echo $v1 $v2
         done
         echo $arr1
         echo $arr2
}

function clean_rc_sqs {
    cd ${CMT_HOME}/vtrack
    cs list -n in-rc --nocolor | awk '{print $1}' | grep in-rc_v  | sed 's/in-rc_//g' | tr '_' '.'  | xargs -L1 python base/RCHelper.py -a end -t
}



function cmtssh {
    # Put keys in all the gateways
     cat ~/.ssh/config | grep "Host \|ProxyJump" | paste - - | awk '{print $2, $NF}' | sed 's/.*\*\.//g' | sed 's/\.cmt / /g' | while read line
     do
         (
             reg=$(echo $line | awk '{print $1}')
             host=$(echo $line | awk '{print $2}')
             ip=$(host ${host} | head -n 1 | awk '{print $4}')
             name=$(aws ec2 describe-instances --filter Name=ip-address,Values=${ip} --query "Reservations[*].Instances[*].[Tags[?Key=='Name'].Value]" --output text --region ${reg})
             if [[ $(echo ${name} | wc -c) -gt 1 ]]; then
                 echo ${reg} '->' ${host} '->' ${name} '->' $(echo ${name} | wc -c)
                 cmtaws ec2 putsshkey -y --pubkey ${CMT_ID_RSA}.pub -n ${name} -r ${reg}
             fi
         ) &
     done
     wait

}

function cmtsyncrepo {
    host='rc-par'
    if [[ ($# -eq 2) ]]; then
        host=$2
    fi
    echo "rsync --exclude '.git' --exclude '.venv' -avz  $(awsdns $host):\~/cmt/$1/ ~/cmt/$1/"
    rsync --exclude '.git' --exclude '.venv' -avz  $(awsdns $host):\~/cmt/$1/ ~/cmt/$1/

}

function cmtsync {
    host='rc-par'
    if [[ ($# -eq 1) ]]; then
        host=$1
    fi
    SYNCDIR=$(print -rD $PWD)
    echo "rsync --exclude '.git' --exclude '.venv*' --exclude '*.pyc' -avz  $(awsdns $host):\"${SYNCDIR}/\" ./"
    rsync --exclude '.git' --exclude '.venv*' --exclude '*.pyc' -avz  $(awsdns $host):${SYNCDIR}/ ./
}

function cmtpush {
    host='rc-par'
    if [[ ($# -eq 1) ]]; then
        host=$1
    fi
    SYNCDIR=$(print -rD $PWD)
    echo "rsync --exclude '.git' --exclude '.venv*' --exclude '*.pyc' -avz  ./ $(awsdns $host):\"${SYNCDIR}/\""
    rsync --exclude '.git' --exclude '.venv*' --exclude '*.pyc' -avz  ./ $(awsdns $host):${SYNCDIR}/
}


alias cmtsyncap="cmtsyncrepo accel-pipeline"
alias cmtsyncmm="cmtsyncrepo mapmatch"
alias cmtsyncvt="cmtsyncrepo vtrack"
alias cmtsyncda="cmtsyncrepo dataanalytics"


function rc-helper-tunnel {
    PGUSER=vtrack PGDATABASE=vtrack PGHOST=localhost cmtaws tunnel --bind-port-env-var PGPORT localhost:0:rc-dbro-service.us-east-1.cmt:5432 rc-utility.us-east-1.cmtaws -- rc-helper "$@"
}
